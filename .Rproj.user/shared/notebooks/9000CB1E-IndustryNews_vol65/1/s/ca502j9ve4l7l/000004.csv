"0","library(rvest)"
"0","library(tidyverse)"
"0","library(tidytext)"
"0","library(wordcloud2)"
"0","library(stringr)"
"0",""
"0",""
"0",""
"0","eng_news <- function(term) {"
"0",""
"0","  html_dat <- read_html(paste0(""https://news.google.com/search?q="",term,""&hl=en-US&gl=US&ceid=US%3Aen""))"
"0",""
"0","  dat <- data.frame(Link = html_dat %>%"
"0","                      html_nodes('.VDXfz') %>%"
"0","                      html_attr('href')) %>%"
"0","    mutate(Link = gsub(""./articles/"",""https://news.google.com/articles/"",Link))"
"0",""
"0","  news_dat <- data.frame("
"0","    Title = html_dat %>%"
"0","      html_nodes('.DY5T1d') %>%"
"0","      html_text(),"
"0","    Link = dat$Link"
"0","  )"
"0",""
"0","  return(news_dat)"
"0","}"
"0",""
"0","df1 <- eng_news(""enerbility"")"
"0",""
"0","title_text <- df1$Title"
"0",""
"0","title_text <-"
"0","  title_text %>%"
"0","  tolower() %>% # 모두 소문자로 변환"
"0","  str_replace_all(""[[:punct:]]"", """") %>% # 구두점 삭제"
"0","  str_replace_all(""\\W"", "" "") %>% # 알파벳, 숫자, _ 빼고 공백으로 변환"
"0","  str_replace_all(""\\v"", """") %>%  # 수직 탭 (|) 삭제"
"0","  str_replace_all(""(?i)(doosan)"", """") %>% # 대소문자 관계없이 doosan 삭제"
"0","  str_replace_all(""(?i)(enerbility)"", """") %>%"
"0","  str_replace_all(""(?i)(heavy)"", """")"
"0",""
"0","title_text <-"
"0","  title_text %>%"
"0","  tibble() # no=1:nrow(df1), text = title_text"
"0",""
"0","names(title_text)[1] <- ""title"""
"0",""
"0","title_text <-"
"0","  title_text %>%"
"0","  unnest_tokens(word, title) %>%"
"0","  anti_join(stop_words) %>%"
"0","  count(word, sort = TRUE)"
"1","[38;5;252mJoining, by = ""word""[39m
"
"0","names(title_text) <- c(""words"", ""freq"")"
"0",""
"0","title_text %>% wordcloud2(minRotation = -pi/6,"
"0","                          maxRotation = -pi/6,"
"0","                          rotateRatio = 1)"
